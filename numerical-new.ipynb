{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import cross_val_score, train_test_split, GridSearchCV\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.utils import resample\n",
    "import time\n",
    "from alchemy_conn import alchemy_engine\n",
    "\n",
    "#inherently multiclass:\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.neighbors import RadiusNeighborsClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "#multiclass as One-vs-One:\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.gaussian_process import GaussianProcessClassifier #set multi_class=\"one_vs_one\"\n",
    "\n",
    "#multiclass as One-Vs-All:\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.linear_model import Perceptron\n",
    "\n",
    "import pickle as pickle\n",
    "\n",
    "np.random.seed(1000) #to get consistent results every time--used to test hyperparameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from my_libraries import *\n",
    "capstone_folder, images_folder = folders()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#read data into df\n",
    "\n",
    "#use imported alchemy_conn program to generate sqlalchemy connection to summitsdb\n",
    "engine = alchemy_engine()\n",
    "\n",
    "#load summits table into pandas df\n",
    "df = pd.read_sql_query('''SELECT * FROM summits WHERE type_str IN ('mount', 'mountain', 'peak') ORDER BY summit_id;''', con=engine)\n",
    "    # df = pd.read_csv('~/dsi/Capstone/summits.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "mountain_mask = df.type_str == 'mountain'\n",
    "mount_mask = df.type_str == 'mount'\n",
    "peak_mask = df.type_str == 'peak'\n",
    "\n",
    "df_mountain = df[mountain_mask]\n",
    "df_mount = df[mount_mask]\n",
    "df_peak = df[peak_mask]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len(X3=15000, len(y3)=15000)\n"
     ]
    }
   ],
   "source": [
    "numrows_in_each_class = 5000\n",
    "\n",
    "# print(\"Before resampling: #mount={}, #mountains={}, #peaks={}\"\n",
    "#       .format(df_mount.summit_id.count(), df_mountain.summit_id.count(), df_peak.summit_id.count()))\n",
    "\n",
    "num_mount_samples = numrows_in_each_class - df_mount.summit_id.count()\n",
    "num_mountain_samples = numrows_in_each_class - df_mountain.summit_id.count()\n",
    "num_peak_samples = numrows_in_each_class - df_peak.summit_id.count()\n",
    "\n",
    "df_mount_upsample = resample(df_mount, replace=True, n_samples=num_mount_samples)\n",
    "df_mountain_upsample = resample(df_mountain, replace=True, n_samples=num_mountain_samples)\n",
    "df_peak_upsample = resample(df_peak, replace=True, n_samples=num_peak_samples)\n",
    "\n",
    "df3 = pd.concat([df_mountain, df_mountain_upsample, df_mount, df_mount_upsample, df_peak, df_peak_upsample]) \n",
    "\n",
    "X3 = df3[['elevation','isolation', 'prominence']]\n",
    "y3 = df3['type']\n",
    "\n",
    "#normalize features data to range 0 - 1\n",
    "X3_min = X3.min(axis=0) #axis=0 -> up and down columns\n",
    "X3_max = X3.max(axis=0)\n",
    "X3 = (X3 - X3_min) / (X3_max - X3_min)\n",
    "print(\"len(X3={}, len(y3)={})\".format(len(X3), len(y3)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len(X2=16000, len(y2)=16000)\n"
     ]
    }
   ],
   "source": [
    "numrows_in_each_class = 8000\n",
    "\n",
    "# print(\"Before resampling: #mountains={}, #peaks={}\"\n",
    "#       .format(df_mountain.summit_id.count(),  df_peak.summit_id.count()))\n",
    "\n",
    "num_mountain_samples = numrows_in_each_class - df_mountain.summit_id.count()\n",
    "num_peak_samples = numrows_in_each_class - df_peak.summit_id.count()\n",
    "\n",
    "df_mountain_upsample = resample(df_mountain, replace=True, n_samples=num_mountain_samples)\n",
    "df_peak_upsample = resample(df_peak, replace=True, n_samples=num_peak_samples)\n",
    "\n",
    "df2 = pd.concat([df_mountain, df_mountain_upsample, df_peak, df_peak_upsample]) \n",
    "\n",
    "X2 = df2[['elevation','isolation', 'prominence']]\n",
    "y2 = df2['type']\n",
    "\n",
    "#normalize features data to range 0 - 1\n",
    "X2_min = X2.min(axis=0) #axis=0 -> up and down columns\n",
    "X2_max = X2.max(axis=0)\n",
    "X2 = (X2 - X2_min) / (X2_max - X2_min)\n",
    "\n",
    "print(\"len(X2={}, len(y2)={})\".format(len(X2), len(y2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # #split into train, test\n",
    "X3_train, X3_test, y3_train, y3_test = train_test_split(X3, y3, test_size=0.20)\n",
    "X2_train, X2_test, y2_train, y2_test = train_test_split(X2, y2, test_size=0.20)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pick_best_classifier(X_train, X_test, y_train, y_test):\n",
    "    '''\n",
    "    INPUT: four DataFrames\n",
    "    OUTPUT: fitted model (the one with the best f1 score), DataFrame showing score results for each classifier\n",
    "\n",
    "    Loops through multiple classifiers, performing GridSearch with cross validation on training data, and prints train and test results (f1, precision, recall, and accuracy scores) for each. The classifier (fitted on the training data) with the best f1 score is returned.\n",
    "    '''\n",
    "\n",
    "    # these are the classifiers we are testing:\n",
    "    names = ['GradientBoostingClassifier', 'LogisticRegression', 'LogisticRegression--liblinear/ovr', 'DecisionTreeClassifier', 'GaussianNB',\n",
    "             'LinearDiscriminantAnalysis', 'QuadraticDiscriminantAnalysis',\n",
    "             'LinearSVC', 'MLPClassifier', 'RandomForestClassifier', 'SVC',\n",
    "             'SGDClassifier', 'Perceptron']\n",
    "\n",
    "    classifiers = [\n",
    "        GradientBoostingClassifier(),\n",
    "        LogisticRegression(random_state=1, max_iter=1000),\n",
    "        LogisticRegression(random_state=1, max_iter=1000, multi_class='ovr', solver='liblinear'),\n",
    "        DecisionTreeClassifier(random_state=1),\n",
    "        GaussianNB(),\n",
    "        LinearDiscriminantAnalysis(),\n",
    "        QuadraticDiscriminantAnalysis(),\n",
    "        LinearSVC(random_state=1),\n",
    "        MLPClassifier(),\n",
    "        RandomForestClassifier(),\n",
    "        SVC(),\n",
    "        SGDClassifier(),\n",
    "        Perceptron()\n",
    "    ]\n",
    "\n",
    "    #params for GridSearchCV\n",
    "    params=[\n",
    "        {'loss': ['deviance'], 'n_estimators': [50,100,200], 'max_depth': [2,3,5,7],\n",
    "         'criterion': ['friedman_mse'], 'max_features': [None, 'auto', 'sqrt', 'log2']}, #GradientBoostingClassifier\n",
    "        {'multi_class': ['ovr', 'multinomial'],\n",
    "             'solver': ['lbfgs', 'sag', 'saga', 'newton-cg'],\n",
    "             'class_weight': [None, 'balanced']}, #LogisticRegression\n",
    "\n",
    "        {'class_weight': [None, 'balanced']}, #LogisticRegression: liblinear/ovr\n",
    "        {}, #DecisionTreeClassifier\n",
    "        {}, #GaussianNB\n",
    "        {'solver': ['svd', 'lsqr', 'eigen']}, #LinearDiscriminantAnalysis\n",
    "        {}, #QuadraticDiscriminantAnalysis\n",
    "        {'multi_class': ['ovr', 'crammer_singer'], 'class_weight': [None, 'balanced']}, #LinearSVC\n",
    "        {}, #MLPClassifier\n",
    "    #     {}, #RadiusNeighborsClassifier\n",
    "        {}, #RandomForestClassifier\n",
    "        {}, #SVC\n",
    "        {}, #SGDClassifier\n",
    "        {}  #Perceptron\n",
    "            ]\n",
    "\n",
    "    #starting default values\n",
    "    best_test_score = -999.9\n",
    "    worst_test_score = 999.9\n",
    "    best_estimator = ''\n",
    "    longest_time = -1\n",
    "    scores = dict()\n",
    "\n",
    "    #loop through each classifier\n",
    "    # for i in range(1): #for testing\n",
    "    print()\n",
    "    i=0\n",
    "    for i in range(len(params)):\n",
    "        print(\"=============================== {}. {} ==================================\".format(i+1, names[i]))\n",
    "        score_type = 'accuracy'\n",
    "        start_time = time.time()\n",
    "\n",
    "        #GridSearchCV below uses 3 fold cross validation, and searches through parameters in param_grid above for each classifier\n",
    "        gs = GridSearchCV(estimator=classifiers[i], param_grid=params[i], cv=8, n_jobs=-1, scoring=score_type)\n",
    "        gs = gs.fit(X_train, y_train)\n",
    "        seconds = time.time() - start_time\n",
    "        if seconds > longest_time:\n",
    "            longest_time = seconds\n",
    "            longest_time_estimator = names[i]\n",
    "\n",
    "        #predict results (y_pred) with best_estimator (one with best parameters from GridSearchCV)\n",
    "        model = gs.best_estimator_\n",
    "        model.fit(X_train, y_train)\n",
    "        y_pred = model.predict(X_test)\n",
    "\n",
    "        #calculate and print scores\n",
    "        f1 = f1_score(y_test, y_pred, average='weighted')\n",
    "        precision = precision_score(y_test, y_pred, average='weighted')\n",
    "        recall = recall_score(y_test, y_pred, average='weighted')\n",
    "        accuracy = accuracy_score(y_test, y_pred)\n",
    "        test_score = f1 #f1 used to rank classifiers\n",
    "\n",
    "        #print results\n",
    "        print(\"TEST {} score: {}\".format(score_type, test_score))\n",
    "        print(\"TRAIN best {} score={}\".format(score_type, gs.best_score_))\n",
    "        if params[i] == {}:\n",
    "            cur_params = \"params: default used\"\n",
    "        else:\n",
    "            cur_params = \"best params: {}\".format(gs.best_params_)\n",
    "        print(cur_params)\n",
    "        print(\"#seconds for GridSearchCV for this classifier={}\\n\".format(seconds))\n",
    "        print(\"TEST scores:\\nf1: {}\\nprecision: {}\\nrecall: {}\\naccuracy: {}\\n\".format(f1, precision, recall, accuracy))\n",
    "        print(\"\\nconfusion matrix:\\n    TN    FP\\n    FN    TP\\n{}\\n\".format(confusion_matrix(y_test, y_pred)))\n",
    "\n",
    "        #store scores for printing summary later\n",
    "        scores[names[i]] = (f1, precision, recall, accuracy)\n",
    "\n",
    "        #record best and worst results from all classifiers\n",
    "        if test_score > best_test_score:\n",
    "            second_best_estimator = best_estimator\n",
    "            second_best_score = best_test_score\n",
    "            best_test_score = test_score\n",
    "            best_estimator = names[i]\n",
    "            best_params = cur_params\n",
    "            best_estimator_seconds = seconds\n",
    "            best_model = model\n",
    "        if test_score < worst_test_score:\n",
    "            worst_test_score = test_score\n",
    "            worst_estimator = names[i]\n",
    "            worst_params = cur_params\n",
    "            worst_estimator_seconds = seconds\n",
    "\n",
    "    #after running each classifer, print summary results\n",
    "    print()\n",
    "    print(\"++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\")\n",
    "    print(\"=============================== SUMMARY ==================================\")\n",
    "    print(\"best_estimator={}\\nparams={}\\nf1 test score={}\\n#seconds={}\".format(best_estimator, best_params, best_test_score, best_estimator_seconds))\n",
    "    print(\"\\nsecond_best_estimator: {}, f1 score: {}\".format(second_best_estimator, second_best_score))\n",
    "    print(\"\\nworst_estimator={}, #seconds={}, params={}, f1 test score={}\".format(worst_estimator, worst_estimator_seconds, worst_params, worst_test_score))\n",
    "    print(\"\\nestimator that took most time: {}, seconds: {}\".format(longest_time_estimator, longest_time))\n",
    "\n",
    "    #print scores summary: rows are each classifer, columns f1, precision, recall, accuracy\n",
    "    scores = pd.DataFrame(scores).T\n",
    "    scores.columns = ['f1', 'precision', 'recall', 'accuracy']\n",
    "    scores = scores.sort_values('f1', ascending=False)\n",
    "    print(\"\\nSummary of results:\\n{}\".format(scores))\n",
    "\n",
    "    print(\"++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\\n\")\n",
    "    return best_model, scores #returns classifer with best f1 score and best parameters from GridSearchCV\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=============================== 1. GradientBoostingClassifier ==================================\n",
      "TEST accuracy score: 0.8321897124204389\n",
      "TRAIN best accuracy score=0.816171875\n",
      "best params: {'criterion': 'friedman_mse', 'loss': 'deviance', 'max_depth': 7, 'max_features': None, 'n_estimators': 200}\n",
      "#seconds for GridSearchCV for this classifier=29.648138284683228\n",
      "\n",
      "TEST scores:\n",
      "f1: 0.8321897124204389\n",
      "precision: 0.8322050902718017\n",
      "recall: 0.8321875\n",
      "accuracy: 0.8321875\n",
      "\n",
      "\n",
      "confusion matrix:\n",
      "    TN    FP\n",
      "    FN    TP\n",
      "[[1324  264]\n",
      " [ 273 1339]]\n",
      "\n",
      "=============================== 2. LogisticRegression ==================================\n",
      "TEST accuracy score: 0.5883757084829228\n",
      "TRAIN best accuracy score=0.58921875\n",
      "best params: {'class_weight': 'balanced', 'multi_class': 'ovr', 'solver': 'lbfgs'}\n",
      "#seconds for GridSearchCV for this classifier=8.156395196914673\n",
      "\n",
      "TEST scores:\n",
      "f1: 0.5883757084829228\n",
      "precision: 0.5884105886521579\n",
      "recall: 0.5884375\n",
      "accuracy: 0.5884375\n",
      "\n",
      "\n",
      "confusion matrix:\n",
      "    TN    FP\n",
      "    FN    TP\n",
      "[[915 673]\n",
      " [644 968]]\n",
      "\n",
      "=============================== 3. LogisticRegression--liblinear/ovr ==================================\n"
     ]
    }
   ],
   "source": [
    "best_model, scores = pick_best_classifier(X2_train, X2_test, y2_train, y2_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model, scores = pick_best_classifier(X3_train, X3_test, y3_train, y3_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model3 = GradientBoostingClassifier(criterion='friedman_mse', loss='deviance', max_depth=7, max_features='sqrt', n_estimators=200)\n",
    "model2 = GradientBoostingClassifier(criterion='friedman_mse', loss='deviance', max_depth=7, max_features='sqrt', n_estimators=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model3.fit(X3_train, y3_train)\n",
    "model2.fit(X2_train, y2_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result3 = model3.predict(X3)\n",
    "result2 = model2.predict(X2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result3L = []\n",
    "for r in result3:\n",
    "    if r == 0:\n",
    "        result3L.append('mount')\n",
    "    if r == 1:\n",
    "        result3L.append('mountain')\n",
    "    if r == 2:\n",
    "        result3L.append('peak')\n",
    "result3L = np.array(result3L)\n",
    "result3L[:10], result3[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result2L = []\n",
    "for r in result2:\n",
    "    if r == 0:\n",
    "        result2L.append('mount')\n",
    "    if r == 1:\n",
    "        result2L.append('mountain')\n",
    "    if r == 2:\n",
    "        result2L.append('peak')\n",
    "result2L = np.array(result2L)\n",
    "result2L[:10], result2[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(capstone_folder + \"pickled_images_labels/labels_type_GBC3.pkl\", 'wb') as f:\n",
    "    pickle.dump(result3L, f, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(capstone_folder + \"pickled_images_labels/labels_type_GBC2.pkl\", 'wb') as f:\n",
    "    pickle.dump(result2L, f, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
